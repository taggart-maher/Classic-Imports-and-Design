from selenium import webdriver 
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
import pandas as pd
import csv
import urllib.request
from lxml import html
import time
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.chrome.options import Options
from pprint import pprint as pp
from tabulate import tabulate

login_url = 'https://www.savoyhouse.com/'

writefile = open('Classic-Imports-and-Design\python webscrape\output.csv', 'w+', encoding='UTF8', newline='')
output_file = csv.writer(writefile, delimiter=",")

options = webdriver.ChromeOptions()
options.add_experimental_option('excludeSwitches', ['enable-logging'])
options.add_argument('--disable-blink-features=AutomationControlled')
driver = webdriver.Chrome('Classic-Imports-and-Design\python webscrape\chromedriver.exe', options=options)

#login
print('\n\nmklatsky\n\n23Monday!01\n\n')
driver.get(login_url)
print("Please log into the wholesale account.")
input()

file_header = ['sku', 'Name', 'Categories']
output_file.writerow(file_header)
fails = []

active = True
while active == True:
    print('PLEASE SELECT NEW & ACTIVE PRODUCTS ONLY!!!')
    print('Navigate to a category. Then type the name. Type 000 to stop.')
    cat = input()
    if cat == '000':
        active = False
        continue
    grid = driver.find_elements(By.CLASS_NAME, 'catalog-item')
    for element in grid:
        data = []
        price = float(element.find_element(By.CLASS_NAME, 'catalog-item-price').text.lstrip('$').replace(',',''))
        sku = element.find_element(By.CLASS_NAME, 'catalog-item-number').text
        print(sku + ' | ' + str(price))
        if price <= 75:
            continue
        data.append(sku)
        data.append(cat)
        output_file.writerow(data)

driver.quit()
print('F I N !')
writefile.close()

#element_present = EC.presence_of_element_located((By.CLASS_NAME, 'product-item-link'))
#WebDriverWait(driver, 6).until(element_present)
#driver.get(url)
#time.sleep()
#content = driver.page_source
#soup = BeautifulSoup(content, "html.parser")
#tree = html.fromstring(driver.page_source) 

#driver.find_elements(By.XPATH, XPATH)
#driver.find_elements(By.CLASS_NAME, CLASSNAME)
#soup.find("TAG-TYPE", class_="CLASS-NAME").text

#urllib.request.urlretrieve(IMG-URL, FILENAME) 

#output_file.writerow(data)